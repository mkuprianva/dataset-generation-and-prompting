{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Dataset Generation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Why do we need to know this\n",
    "\n",
    "#### Fine tuning models\n",
    "\n",
    " - What should we be fine tuning\n",
    "   -- Fine-tuning smaller pre-trained models, like LLaMA or Phi-3, can significantly enhance their performance on specific tasks by allowing them to adapt to domain-specific data, leading to improved accuracy and relevance. This customization can transform a general-purpose model into a highly specialized tool, providing value in diverse fields such as healthcare, finance, and customer service. \n",
    "\n",
    "   -- Fine-tuning smaller, domain-specific models can significantly reduce the risk of hallucinations, which are factually incorrect or misleading outputs. This is because these models are trained on high-quality, relevant data, allowing them to produce outputs more aligned with the factual nuances of the domain2. By focusing on targeted learning, these models are less likely to generate inaccurate information compared to larger, general-purpose models. This targeted approach helps in mitigating the generation of misinformation, toxicity, and stereotypes.\n",
    "   \n",
    "   -- However, fine-tuning also carries risks, such as overfitting to the fine-tuning data, which can reduce the model's generalization capability. There is also the potential for introducing biases from the fine-tuning dataset, resulting in unethical or unreliable outputs. These risks underscore why foundational models like GPT-4 are often left untouched. \n",
    "\n",
    "   -- Foundational models are designed to be highly versatile and capable of performing a wide range of tasks without modification. Fine-tuning them could compromise their broad applicability and the integrity of their underlying architectures, making them less effective at handling the diverse array of tasks they are meant to perform.\n",
    "\n",
    "### Do's and Don'ts of fine tuning\n",
    "\n",
    "### Manual vs automated dataset generation\n",
    "\n",
    "### Train the trainer and knoweldge distillation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
