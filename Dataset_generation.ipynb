{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Generation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Why do we need to know this\n",
    "\n",
    "#### Fine tuning models\n",
    "\n",
    " - What should we be fine tuning\n",
    "   -- Fine-tuning smaller pre-trained models, like LLaMA or Phi-3, can significantly enhance their performance on specific tasks by allowing them to adapt to domain-specific data, leading to improved accuracy and relevance. This customization can transform a general-purpose model into a highly specialized domain expert that can be used in multiple different settings while decreasing compute cost.\n",
    "\n",
    "[Forbes - Small Language Models More Effectie and Efficient for Enterprise AI](<references/Small Language Models – More Effective And Efficient For Enterprise AI.html>)\n",
    "\n",
    "   <a href=\"references/Small Language Models – More Effective And Efficient For Enterprise AI.html\" target=\"_blank\">Forbes Article </a>\n",
    "\n",
    "   -- Fine-tuning smaller, domain-specific models can significantly reduce the risk of hallucinations, which are factually incorrect or misleading outputs. This is because these models are trained on high-quality, relevant data, allowing them to produce outputs more aligned with the factual nuances of the domain2. By focusing on targeted learning, these models are less likely to generate inaccurate information compared to larger, general-purpose models. This targeted approach helps in mitigating the generation of misinformation, toxicity, and stereotypes.\n",
    "   [Honest AI: Fine-Tuning \"Small\" Language Models to Say \"I Don't Know\", and Reducing Hallucinations in RAG](<references/Honest AI.pdf>)\n",
    "   \n",
    "   ![alt text](references/ComparisionOfHallucinations.png \"Comparision of model size vs hallucination risk\")\n",
    "\n",
    "   -- However, fine-tuning also carries risks, such as overfitting to the fine-tuning data, which can reduce the model's generalization capability. There is also the potential for introducing biases from the fine-tuning dataset, resulting in unethical or unreliable outputs. These risks underscore why foundational models like GPT-4 are often left untouched. \n",
    "\n",
    "   -- Foundational models are designed to be highly versatile and capable of performing a wide range of tasks without modification. Fine-tuning them could compromise their broad applicability and the integrity of their underlying architectures, making them less effective at handling the diverse array of tasks they are meant to perform.\n",
    "\n",
    "### Do's and Don'ts of fine tuning\n",
    "\n",
    "### Manual vs automated dataset generation\n",
    "\n",
    "### Train the trainer and knoweldge distillation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://google.com\">Google</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'base (Python 3.12.2)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "%%html\n",
    "<h1>This is a test</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
