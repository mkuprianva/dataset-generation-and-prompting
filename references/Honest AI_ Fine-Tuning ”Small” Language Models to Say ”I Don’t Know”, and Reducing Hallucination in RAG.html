<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG</title>
<!--Generated on Sat Oct 12 05:35:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Large Language Models (LLM),  Retrieval Augmented Generation (RAG),  Knowledge Graph,  Search" lang="en" name="keywords"/>
<base href="/html/2410.09699v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S1" title="In Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S2" title="In Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S3" title="In Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodologies</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S3.SS1" title="In 3. Methodologies â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S3.SS2" title="In 3. Methodologies â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Scoring</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S3.SS3" title="In 3. Methodologies â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>RAG</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S3.SS4" title="In 3. Methodologies â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S3.SS5" title="In 3. Methodologies â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Hybrid Approach</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S4" title="In Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S5" title="In Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S6" title="In Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S7" title="In Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Acknowledgments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#A1" title="In Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Research Methods</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#A1.SS1" title="In Appendix A Research Methods â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Prompt of Generating Answers with Domain in JSON format</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#A1.SS2" title="In Appendix A Research Methods â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>An example of hallucination</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<div class="ltx_para" id="p1">
<span class="ltx_ERROR undefined" id="p1.1">\DeclareUnicodeCharacter</span>
<p class="ltx_p" id="p1.2">266B<span class="ltx_ERROR undefined" id="p1.2.1">\textmusicalnote</span></p>
</div>
<h1 class="ltx_title ltx_title_document">Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xinxi Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Independent Researcher</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:xc336@cornell.edu">xc336@cornell.edu</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Li Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id2.1.id1">Independent Researcher</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:li@liwang.info">li@liwang.info</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Wei Wu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id3.1.id1">Independent Researcher</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:drweiwu@outlook.com">drweiwu@outlook.com</a>
</span></span></span>
<span class="ltx_author_before">,Â </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qi Tang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Independent Researcher</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:qitang@closeby-ai.com">qitang@closeby-ai.com</a>
</span></span></span>
<span class="ltx_author_before">Â andÂ </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yiyao Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id5.1.id1">Independent Researcher</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:info@yiyaoliu.com">info@yiyaoliu.com</a>
</span></span></span>
</div>
<div class="ltx_dates">(2024)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id6.id1">Hallucination is a key roadblock for applications of Large Language Models (LLMs), particularly for enterprise applications that are sensitive to information accuracy. To address this issue, two general approaches have been explored: Retrieval-Augmented Generation (RAG) to supply LLMs with updated information as context, and fine-tuning the LLMs with new information and desired output styles. In this paper, we propose Honest AI: a novel strategy to fine-tune â€smallâ€ language models to say â€I donâ€™t knowâ€ to reduce hallucination, along with several alternative RAG approaches. The solution ranked 1st in Task 2 for the false premise question<span class="ltx_note ltx_role_footnote" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The Meta CRAG Challenge has over 2000 participants with over 5500 submissions.</span></span></span>. The alternative approaches include using RAG with search engine and knowledge graph results, fine-tuning base LLMs with new information and combinations of both approaches. Although all approaches improve the performance of the LLMs, RAG alone does not significantly improve the performance and fine-tuning is needed for better results. Finally, the hybrid approach achieved the highest score in the CRAG benchmark <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib28" title="">2024</a>)</cite>. In addition, our approach emphasizes the use of relatively small models with fewer than 10 billion parameters, promoting resource efficiency.
</p>
</div>
<div class="ltx_keywords">Large Language Models (LLM), Retrieval Augmented Generation (RAG), Knowledge Graph, Search
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">journalyear: </span>2024</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">doi: </span></span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">conference: </span>2024 KDD Cup Workshop for Retrieval Augmented Generation at the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining; Aug 25â€“29,
2024; Barcelona, Spain</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id5"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">â€ </sup><span class="ltx_note_type">ccs: </span>Computing methodologiesÂ Natural language generation</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Large Language Models (LLMs), as a type of foundation models with general language capabilities, have eclipsed traditional Natural Language Processing (NLP) models that focus on specific tasks in majority of NLP applications since the inception of GPT
<cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib2" title="">2020b</a>)</cite>. Supervised fine tuning (SFT) using
labeled data and reinforcement learning from human feedback
(RLHF) using preference data have proven effective in further enhancing LLMsâ€™ performance and alignment (e.g., ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(Ouyang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib18" title="">2022</a>)</cite> for question answering applications). However, LLMs suffer from hallucination, which hinders their application in accuracy-sensitive scenarios, such as the enterprise applications.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To alleviate the hallucinations of LLMs, several approaches have been proposed, including retrieval-augmented generations (RAGs) <cite class="ltx_cite ltx_citemacro_citep">(Lewis etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib13" title="">2020</a>)</cite> and fine-tuning with domain-specific knowledge. In this paper, we summarize the approaches we tried in the 2024 Meta KDD Cup competition with Comprehensive RAG Benchmark (CRAG) data <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib28" title="">2024</a>)</cite>. Since the CRAG benchmark focuses on difficult problems for LLMs, vanilla LLMs fail to perform well right out of the box. It turns out that RAG alone is not enough to alleviate hallucination in the benchmark and fine-tuning is needed to achieve higher accuracy. Our results show that the hybrid approach using both RAG and fine tuning performs best in CRAG. With those approaches combined, our team, Team Future, ranked high in each task of the competition and won first place in the false premise question in Task 2 (Fig. Â <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S1.F1" title="Figure 1 â€£ 1. Introduction â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The review of related work is summarized in section 2, followed by a description of methodologies in section 3. The results are shown in section 4, with conclusions and future works discussed in section 5 and 6.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="303" id="S1.F1.g1" src="extracted/5849219/ranking.jpg" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Team Future in Meta CRAG Challenge 2024 Winner List</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">LLMs are very good at memorizing the content used for their pretraining. However, there are many
drawbacks to using the memorization capability directly for Question Answering (QA) tasks.
For example, depending on the
size of the model, the quality of the pretraining data, and the type
of questions, LLMsâ€™ memorization capability can be very limited and difficult to control. LLMs
are also challenging to update except through retraining or fine-tuning, so they cannot handle questions on recent events if deployed. Arguably, the most
problematic drawback of using LLMs for QA is that they can hallucinate, especially when the models
are unsure if the context contains the information needed to answer a question.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Fine-tuning is a straightforward way to update the knowledge of LLMs when new information becomes available. However, due to the scarcity of GPUs and limited access to high-quality data, it is not feasible for many use cases. Furthermore, the behavior of fine-tuning language models is not well studied, and fine-tuning with poor data or practices might decrease the modelâ€™s capabilities, causing modality collapse unless more complex methods such as RLHF are used <cite class="ltx_cite ltx_citemacro_citep">(Ouyang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib18" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">Retrieval-Augmented Generation (RAG) <cite class="ltx_cite ltx_citemacro_citep">(Lewis etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib13" title="">2020</a>; Gao etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib9" title="">2024</a>)</cite>
is a popular method to address the shortcomings of LLMs, by augmenting them with
non-parametric data sources, and leveraging LLMsâ€™ powerful in-context learning <cite class="ltx_cite ltx_citemacro_citep">(Brown etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib3" title="">2020a</a>)</cite>
capability. <cite class="ltx_cite ltx_citemacro_citep">(Gao etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib9" title="">2024</a>)</cite> grouped
the approaches of using RAG for LLMs into three categories: Naive RAG,
Advanced RAG <cite class="ltx_cite ltx_citemacro_citep">(Ma etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib15" title="">2023</a>; Zheng etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib32" title="">2024</a>)</cite>, and Modular RAG <cite class="ltx_cite ltx_citemacro_citep">(Yu etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib30" title="">2023</a>; Shao etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib23" title="">2023</a>)</cite>.
Naive and Advanced RAG approaches are widely used in
practice due their simplicity and low development cost. These approaches generally
consist of three parts: curating non-parametric databases, retrieving relevant snippets
from the databases given the query, and generating responses using LLMs through in-context learning and prompt engineering with the related snippets.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">While the research on combining LLMs and RAG for QA mainly focuses on text, there
is also research exploring the use of resources beyond text, such as images <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib5" title="">2022</a>)</cite>, audio <cite class="ltx_cite ltx_citemacro_citep">(Zhao etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib31" title="">2023</a>)</cite>, video <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib27" title="">2023</a>)</cite>, and code <cite class="ltx_cite ltx_citemacro_citep">(Nashid etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib17" title="">2023</a>)</cite> to enhance the capabilities of language models.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">There are various efforts to create RAG benchmarks and proposing
appropriate evaluation metrics in recent years. <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib28" title="">2024</a>)</cite> is one
of the recent ones, which forms the foundation of this paper. <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib28" title="">2024</a>)</cite>
created a factual question answering benchmark of 4,409 question-answer pairs and
mock APIs to simulate web and Knowledge Graph (KG) searches. It also proposes an evaluation mechanism that distinguishes between hallucinations and missing answers, and assigns a higher penalty to hallucinations. <cite class="ltx_cite ltx_citemacro_citep">(Chen etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib4" title="">2024</a>)</cite> created a RAG evaluation benchmark in both
English and Chinese, and analyzed different LLMs from 4 aspects: noise robustness,
negative rejection, information integration, and counterfactual. They found
that LLMs demonstrate a certain degree of noise robustness, but struggle significantly
in other aspects.</p>
</div>
<div class="ltx_para" id="S2.p6">
<p class="ltx_p" id="S2.p6.1">Apart from specific RAG datasets, there are many existing QA datasets that include context passages for each question.
These datasets can also be used for RAG experiments,
and cover a wide range of questions such as multiple-choice QA
<cite class="ltx_cite ltx_citemacro_citep">(Pang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib19" title="">2022</a>; Clark etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib6" title="">2018</a>; Talmor etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib24" title="">2019</a>)</cite>,
single-hop QA <cite class="ltx_cite ltx_citemacro_citep">(Kwiatkowski etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib12" title="">2019</a>; Joshi etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib11" title="">2017</a>; Rajpurkar etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib20" title="">2016</a>)</cite>,
multi-hop QA <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib29" title="">2018</a>; Ho etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib10" title="">2020</a>; Trivedi etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib25" title="">2022</a>)</cite>,
and domain-specific QA <cite class="ltx_cite ltx_citemacro_citep">(Dasigi etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib7" title="">2021</a>; MÃ¶ller etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib16" title="">2020</a>; Wang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib26" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Methodologies</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we will first introduce the CRAG dataset and then describe the main approaches we have tried, including native RAG, fine-tuning, and hybrid approaches.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Dataset</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The CRAG dataset includes five question domains (e.g. movie, sports, and etc.) with varying levels of complexity, ranging from straightforward facts to those requiring reasoning (e.g. false premise and multi-hop reasoning) <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib28" title="">2024</a>)</cite>. It also considers facts with different levels of timeliness (e.g. real-time, fast-changing, slow-changing, and stable). Figure 2 illustrates the distribution of question types across different domains and timeliness categories. Notably, the characteristics of question distributions in the movie and finance domains differ significantly. Real-time and fast-changing questions necessitate access to relevant, up-to-date data sources and effective RAG implementations. In contrast, the movie domain contains more static questions than the finance domain, which may be easier to address.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="395" id="S3.F2.g1" src="extracted/5849219/Fig1_RAG.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>The Distribution of Questions in Different Domains and Types of Timeliness </figcaption>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="453" id="S3.F3.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Fine-tuning Process</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Our solution ranked first in Task 2 for false premise questions. A false premise question is defined as a question with a false assumption. For example, â€Whatâ€™s the name of Taylor Swiftâ€™s rap album before she transitioned to pop?â€ This is a false premise question because Taylor Swift didnâ€™t release any rap albums, and the expected answer is â€invalid questionâ€ <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib28" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">In this competition, we chose a divide-and-conquer approach to design a RAG system to tackle different types of questions. We assumed that categorizing these questions could be done easily in practice. The objective is to identify the key parametric differences, such as varying prompts, parsing techniques, chunking sizes, top-k thresholds, and rule designs, to optimize performance and achieve a higher score during implementation.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Scoring</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The overall score is a macro-average across all domains. The scoring system awards scores based on the quality of the response, and penalizes hallucination <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib28" title="">2024</a>)</cite>. If the response is perfect, it receives 1 point. If the response is acceptable, it receives 0.5 points. If the response is missing, i.e. â€I donâ€™t knowâ€, it receives 0 points. If the response is incorrect, it receives -1 point.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>RAG</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">RAG <cite class="ltx_cite ltx_citemacro_citep">(Lewis etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib13" title="">2020</a>; Gao etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib9" title="">2024</a>)</cite> is a popular approach to alleviate the hallucinations of LLMs. There are various architectures for RAG in real applications, including advanced RAG techniques. In this paper, we explored several naive RAG approaches. Generally, naive RAG selects the highest cosine similarity results from a vector database and supplies the context as inputs for LLMs. There are multiple ways to find the relevant information to implement RAG. We tried using naive cosine similarity and using LLMs as retrieval and ranking models to find relevant information for each web page.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Furthermore, as an extreme case, we also tried using the state-of-the-art Gemini 1.5 pro model with a 1 million token context window <cite class="ltx_cite ltx_citemacro_citep">(Reid etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib21" title="">2024</a>)</cite> to supply all the retrieved web pages to LLMs. These models with long context windows are promising because they eliminate the need to truncate information from retrieved results. However, with vanilla Gemini 1.5 pro and raw retrieved web pages, we achieved similar results like vanilla RAG with severe hallucination, which is disappointing. Therefore, we didnâ€™t further investigate more advanced RAG techniques. An example of hallucination is shown in Appendix A.2.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">These results for RAG are summarized in Section 4.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Fine-tuning</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Our initial investigation revealed that
answers in most categories are very challenging, and the evaluation metric heavily penalizes
hallucinations. Therefore, it is better for the model to be â€honestâ€ about its limits by
replying â€i donâ€™t knowâ€ in cases of uncertainty, rather than providing wrong answers.
However, it is non-trivial to assess LLMsâ€™ confidence level reliably <cite class="ltx_cite ltx_citemacro_citep">(Lin etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib14" title="">2024</a>)</cite>.
Instead, we hypothesize that by explicitly teaching LLMs to reply â€i donâ€™t knowâ€ to challenging questions
while providing real answers for easy questions, LLMs may be able to learn the ability
to distinguish between challenging and easy questions.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">To test this hypothesis,
we decided to use the QLoRA <cite class="ltx_cite ltx_citemacro_citep">(Dettmers etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib8" title="">2023</a>)</cite>
technique to fine-tune the <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS4.p2.1.1">Llama-2-7b-chat</code> in 4-bit precision and optimize VRAM usage,
due to limited GPU resources.
More specifically, as it is shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S3.F3" title="Figure 3 â€£ 3.1. Dataset â€£ 3. Methodologies â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_tag">3</span></a>, we used the training data provided by the organizer, reserved 250
instances for testing, and made a few modifications to the rest of the data. If the
<code class="ltx_verbatim ltx_font_typewriter" id="S3.SS4.p2.1.2">question_type</code> is <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS4.p2.1.3">comparison</code> or <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS4.p2.1.4">false_premise</code>, or the
<code class="ltx_verbatim ltx_font_typewriter" id="S3.SS4.p2.1.5">answer</code> is â€yesâ€, â€noâ€, â€trueâ€ or â€falseâ€, we do not modify the answers to the questions;
otherwise, we replace the original answer with â€i donâ€™t knowâ€. Then we used the modified data
to fine-tune the model.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">We use Alpha=16, r=64, and Dropout=0.1 for QLoRA. Additionally, we
use a batch size of 8, a learning rate of 0.0002, a weight decay of 0.001, and fine-tune
the model for 5 epochs. We evaluate the fine-tuned model on the 250 withheld questions, using
the offline evaluation script provided by the organizer.</p>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1">We also experimented with <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS4.p4.1.1">Meta-Llama-3-8B-Instruct</code>, which performed
consistently worse.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="712" id="S3.F4.g1" src="extracted/5849219/hybrid_v1.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Hybrid Approach </figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>Hybrid Approach</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">With the fine-tuned model mentioned in the above section, we achieved high rankings in the competition, and first place for the false premise question type in Task 2: Knowledge Graph and Web Retrieval. However, the method did not fully utilize the additional knowledge from the web search results and the knowledge graph. To improve the results, we developed the following hybrid approach to better utilize the knowledge in certain domains.</p>
</div>
<div class="ltx_para" id="S3.SS5.p2">
<p class="ltx_p" id="S3.SS5.p2.1">The hybrid approach leveraged the benefits of both the RAG model and the Fine-tuned Question Type model (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S3.F4" title="Figure 4 â€£ 3.4. Fine-tuning â€£ 3. Methodologies â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_tag">4</span></a>). The hybrid approach first utilized the vanilla <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS5.p2.1.1">Meta-Llama-3-8B-Instruct</code> to serve as the RAG model to generate results that include domain and answer information. The next step is to determine if the domain belongs to the movie domain, which has a lower level of hallucination based on the CRAG benchmark results <cite class="ltx_cite ltx_citemacro_citep">(Yang etÂ al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib28" title="">2024</a>)</cite>. If it is in the movie domain and the answer is valid, the answer is used as the final answer. If the answer is invalid, the question is sent to the Fine-tuned Question Type Model, which is good at answering false premise questions, and more modest by answering â€i donâ€™t knowâ€ for other types of questions. Specifically, an answer is considered â€invalidâ€ in two scenarios: when the response is â€invalid question,â€ or when there is a JSON processing error.</p>
</div>
<div class="ltx_para" id="S3.SS5.p3">
<p class="ltx_p" id="S3.SS5.p3.2">For the input of the RAG model, we also applied a Pruner to extract the top <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS5.p3.2.1">k</code> sentences from the web search results. This pruner computes the cosine similarity of each sentence <math alttext="\mathbf{W_{ij}}" class="ltx_Math" display="inline" id="S3.SS5.p3.1.m1.1"><semantics id="S3.SS5.p3.1.m1.1a"><msub id="S3.SS5.p3.1.m1.1.1" xref="S3.SS5.p3.1.m1.1.1.cmml"><mi id="S3.SS5.p3.1.m1.1.1.2" xref="S3.SS5.p3.1.m1.1.1.2.cmml">ğ–</mi><mi id="S3.SS5.p3.1.m1.1.1.3" xref="S3.SS5.p3.1.m1.1.1.3.cmml">ğ¢ğ£</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.1.m1.1b"><apply id="S3.SS5.p3.1.m1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p3.1.m1.1.1.1.cmml" xref="S3.SS5.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p3.1.m1.1.1.2.cmml" xref="S3.SS5.p3.1.m1.1.1.2">ğ–</ci><ci id="S3.SS5.p3.1.m1.1.1.3.cmml" xref="S3.SS5.p3.1.m1.1.1.3">ğ¢ğ£</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.1.m1.1c">\mathbf{W_{ij}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.1.m1.1d">bold_W start_POSTSUBSCRIPT bold_ij end_POSTSUBSCRIPT</annotation></semantics></math> in the document with the question <math alttext="\mathbf{Q_{i}}" class="ltx_Math" display="inline" id="S3.SS5.p3.2.m2.1"><semantics id="S3.SS5.p3.2.m2.1a"><msub id="S3.SS5.p3.2.m2.1.1" xref="S3.SS5.p3.2.m2.1.1.cmml"><mi id="S3.SS5.p3.2.m2.1.1.2" xref="S3.SS5.p3.2.m2.1.1.2.cmml">ğ</mi><mi id="S3.SS5.p3.2.m2.1.1.3" xref="S3.SS5.p3.2.m2.1.1.3.cmml">ğ¢</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p3.2.m2.1b"><apply id="S3.SS5.p3.2.m2.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p3.2.m2.1.1.1.cmml" xref="S3.SS5.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p3.2.m2.1.1.2.cmml" xref="S3.SS5.p3.2.m2.1.1.2">ğ</ci><ci id="S3.SS5.p3.2.m2.1.1.3.cmml" xref="S3.SS5.p3.2.m2.1.1.3">ğ¢</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p3.2.m2.1c">\mathbf{Q_{i}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p3.2.m2.1d">bold_Q start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT</annotation></semantics></math>, after converting them to Sentence-BERT embeddings <cite class="ltx_cite ltx_citemacro_citep">(Reimers and Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#bib.bib22" title="">2019</a>)</cite>. For each top sentence, the following <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS5.p3.2.2">m</code> sentences in the paragraph are appended to the context to enrich the information. To ensure content quality, an additional cosine similarity threshold <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS5.p3.2.3">n</code> is applied, and the answer is used only if this threshold is met for top <code class="ltx_verbatim ltx_font_typewriter" id="S3.SS5.p3.2.4">k</code> sentences.</p>
</div>
<div class="ltx_para" id="S3.SS5.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{cosine\_similarity}(\mathbf{Q_{i}},\mathbf{W_{ij}})=\frac{\mathbf{Q_{i}}%
\cdot\mathbf{W_{ij}}}{\|\mathbf{Q_{i}}\|\|\mathbf{W_{ij}}\|}" class="ltx_Math" display="block" id="S3.E1.m1.4"><semantics id="S3.E1.m1.4a"><mrow id="S3.E1.m1.4.4" xref="S3.E1.m1.4.4.cmml"><mrow id="S3.E1.m1.4.4.2" xref="S3.E1.m1.4.4.2.cmml"><mtext id="S3.E1.m1.4.4.2.4" xref="S3.E1.m1.4.4.2.4a.cmml">cosine_similarity</mtext><mo id="S3.E1.m1.4.4.2.3" xref="S3.E1.m1.4.4.2.3.cmml">â¢</mo><mrow id="S3.E1.m1.4.4.2.2.2" xref="S3.E1.m1.4.4.2.2.3.cmml"><mo id="S3.E1.m1.4.4.2.2.2.3" stretchy="false" xref="S3.E1.m1.4.4.2.2.3.cmml">(</mo><msub id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mi id="S3.E1.m1.3.3.1.1.1.1.2" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml">ğ</mi><mi id="S3.E1.m1.3.3.1.1.1.1.3" xref="S3.E1.m1.3.3.1.1.1.1.3.cmml">ğ¢</mi></msub><mo id="S3.E1.m1.4.4.2.2.2.4" xref="S3.E1.m1.4.4.2.2.3.cmml">,</mo><msub id="S3.E1.m1.4.4.2.2.2.2" xref="S3.E1.m1.4.4.2.2.2.2.cmml"><mi id="S3.E1.m1.4.4.2.2.2.2.2" xref="S3.E1.m1.4.4.2.2.2.2.2.cmml">ğ–</mi><mi id="S3.E1.m1.4.4.2.2.2.2.3" xref="S3.E1.m1.4.4.2.2.2.2.3.cmml">ğ¢ğ£</mi></msub><mo id="S3.E1.m1.4.4.2.2.2.5" stretchy="false" xref="S3.E1.m1.4.4.2.2.3.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.4.4.3" xref="S3.E1.m1.4.4.3.cmml">=</mo><mfrac id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mrow id="S3.E1.m1.2.2.4" xref="S3.E1.m1.2.2.4.cmml"><msub id="S3.E1.m1.2.2.4.2" xref="S3.E1.m1.2.2.4.2.cmml"><mi id="S3.E1.m1.2.2.4.2.2" xref="S3.E1.m1.2.2.4.2.2.cmml">ğ</mi><mi id="S3.E1.m1.2.2.4.2.3" xref="S3.E1.m1.2.2.4.2.3.cmml">ğ¢</mi></msub><mo id="S3.E1.m1.2.2.4.1" lspace="0.222em" rspace="0.222em" xref="S3.E1.m1.2.2.4.1.cmml">â‹…</mo><msub id="S3.E1.m1.2.2.4.3" xref="S3.E1.m1.2.2.4.3.cmml"><mi id="S3.E1.m1.2.2.4.3.2" xref="S3.E1.m1.2.2.4.3.2.cmml">ğ–</mi><mi id="S3.E1.m1.2.2.4.3.3" xref="S3.E1.m1.2.2.4.3.3.cmml">ğ¢ğ£</mi></msub></mrow><mrow id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.1.1.1.1.2.1.cmml">â€–</mo><msub id="S3.E1.m1.1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.1.1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.1.1.2.cmml">ğ</mi><mi id="S3.E1.m1.1.1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.1.1.3.cmml">ğ¢</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.1.1.1.1.2.1.cmml">â€–</mo></mrow><mo id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">â¢</mo><mrow id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.2.cmml"><mo id="S3.E1.m1.2.2.2.2.1.2" stretchy="false" xref="S3.E1.m1.2.2.2.2.2.1.cmml">â€–</mo><msub id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml"><mi id="S3.E1.m1.2.2.2.2.1.1.2" xref="S3.E1.m1.2.2.2.2.1.1.2.cmml">ğ–</mi><mi id="S3.E1.m1.2.2.2.2.1.1.3" xref="S3.E1.m1.2.2.2.2.1.1.3.cmml">ğ¢ğ£</mi></msub><mo id="S3.E1.m1.2.2.2.2.1.3" stretchy="false" xref="S3.E1.m1.2.2.2.2.2.1.cmml">â€–</mo></mrow></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.4b"><apply id="S3.E1.m1.4.4.cmml" xref="S3.E1.m1.4.4"><eq id="S3.E1.m1.4.4.3.cmml" xref="S3.E1.m1.4.4.3"></eq><apply id="S3.E1.m1.4.4.2.cmml" xref="S3.E1.m1.4.4.2"><times id="S3.E1.m1.4.4.2.3.cmml" xref="S3.E1.m1.4.4.2.3"></times><ci id="S3.E1.m1.4.4.2.4a.cmml" xref="S3.E1.m1.4.4.2.4"><mtext id="S3.E1.m1.4.4.2.4.cmml" xref="S3.E1.m1.4.4.2.4">cosine_similarity</mtext></ci><interval closure="open" id="S3.E1.m1.4.4.2.2.3.cmml" xref="S3.E1.m1.4.4.2.2.2"><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">ğ</ci><ci id="S3.E1.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3">ğ¢</ci></apply><apply id="S3.E1.m1.4.4.2.2.2.2.cmml" xref="S3.E1.m1.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.2.2.2.2.1.cmml" xref="S3.E1.m1.4.4.2.2.2.2">subscript</csymbol><ci id="S3.E1.m1.4.4.2.2.2.2.2.cmml" xref="S3.E1.m1.4.4.2.2.2.2.2">ğ–</ci><ci id="S3.E1.m1.4.4.2.2.2.2.3.cmml" xref="S3.E1.m1.4.4.2.2.2.2.3">ğ¢ğ£</ci></apply></interval></apply><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><divide id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2"></divide><apply id="S3.E1.m1.2.2.4.cmml" xref="S3.E1.m1.2.2.4"><ci id="S3.E1.m1.2.2.4.1.cmml" xref="S3.E1.m1.2.2.4.1">â‹…</ci><apply id="S3.E1.m1.2.2.4.2.cmml" xref="S3.E1.m1.2.2.4.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.2.1.cmml" xref="S3.E1.m1.2.2.4.2">subscript</csymbol><ci id="S3.E1.m1.2.2.4.2.2.cmml" xref="S3.E1.m1.2.2.4.2.2">ğ</ci><ci id="S3.E1.m1.2.2.4.2.3.cmml" xref="S3.E1.m1.2.2.4.2.3">ğ¢</ci></apply><apply id="S3.E1.m1.2.2.4.3.cmml" xref="S3.E1.m1.2.2.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.4.3.1.cmml" xref="S3.E1.m1.2.2.4.3">subscript</csymbol><ci id="S3.E1.m1.2.2.4.3.2.cmml" xref="S3.E1.m1.2.2.4.3.2">ğ–</ci><ci id="S3.E1.m1.2.2.4.3.3.cmml" xref="S3.E1.m1.2.2.4.3.3">ğ¢ğ£</ci></apply></apply><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><times id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3"></times><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.1.1.2">ğ</ci><ci id="S3.E1.m1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.1.1.3">ğ¢</ci></apply></apply><apply id="S3.E1.m1.2.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1.2">norm</csymbol><apply id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.2.1.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.1.2">ğ–</ci><ci id="S3.E1.m1.2.2.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.2.2.1.1.3">ğ¢ğ£</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.4c">\text{cosine\_similarity}(\mathbf{Q_{i}},\mathbf{W_{ij}})=\frac{\mathbf{Q_{i}}%
\cdot\mathbf{W_{ij}}}{\|\mathbf{Q_{i}}\|\|\mathbf{W_{ij}}\|}</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.4d">cosine_similarity ( bold_Q start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT , bold_W start_POSTSUBSCRIPT bold_ij end_POSTSUBSCRIPT ) = divide start_ARG bold_Q start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT â‹… bold_W start_POSTSUBSCRIPT bold_ij end_POSTSUBSCRIPT end_ARG start_ARG âˆ¥ bold_Q start_POSTSUBSCRIPT bold_i end_POSTSUBSCRIPT âˆ¥ âˆ¥ bold_W start_POSTSUBSCRIPT bold_ij end_POSTSUBSCRIPT âˆ¥ end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS5.p5">
<p class="ltx_p" id="S3.SS5.p5.1">Based on offline evaluation, this approach improved the total score from 0.073 to 0.86 using results from 300 samples, compared with the Fine-tuned Question Type model. Since our Fine-tuned Question Type model achieved a score of 0.0960 in the online evaluation for Task 3 (without the holdout test), this hybrid approach is expected to achieve a higher score. It has not been evaluated online yet, because the online evaluation system was closed after phase 2.</p>
</div>
<div class="ltx_para" id="S3.SS5.p6">
<p class="ltx_p" id="S3.SS5.p6.1">In addition, the prompt in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#A1.SS1" title="A.1. Prompt of Generating Answers with Domain in JSON format â€£ Appendix A Research Methods â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_tag">A.1</span></a> is used to get the domain name of the question in a JSON output, which can be used for easy processing.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Results</h2>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1. </span>Preliminary test results of Q&amp;A in specific categories</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.1">Domain or Question Type</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2">Prompt Tuning</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.3">RAG</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.4">Accuracy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.5">Hallucination</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.6">Score</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.1">Movie</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2">n/a</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3">w/o</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.4">0.54</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.5">0.46</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.6">0.08</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.1">Movie</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.2">n/a</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.3">w/</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.4">0.52</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.5">0.48</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.6">-0.04</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.1">Movie</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.2">Y*<span class="ltx_note ltx_role_footnote" id="footnotex3"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The prompt tuning strategies detailed in Table 1 predominantly follow instructions such as: answer the question given the context, and regulate the answer format. Prompt examples: answer â€i donâ€™t knowâ€ directly, if â€¦, or â€invalid questionâ€, if â€¦</span></span></span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.3">w/o</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.4">0.26</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.5">0.41</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.4.3.6">-0.32</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.1">Movie</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.2">Y</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.3">w/</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.4">0.35</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.5">0.55</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.5.4.6">-0.19</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.1">Finance</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.2">n/a</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.3">w/o</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.4">0.31</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.5">0.69</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.6.5.6">-0.38</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.6">
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.1">Finance</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.2">n/a</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.3">w/</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.4">0.34</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.5">0.66</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.7.6.6">-0.32</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.7">
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.1">Finance</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.2">Y</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.3">w/o</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.4">0.1</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.5">0.41</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.8.7.6">-0.31</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.8">
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.1">Finance</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.2">Y</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.3">w/</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.4">0.19</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.5">0.55</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.9.8.6">-0.36</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.9">
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.1">Simple</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.2">Y</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.3">w/o</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.4">0.15</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.5">0.39</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.10.9.6">-0.24</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11.10">
<td class="ltx_td ltx_align_center" id="S4.T1.1.11.10.1">Simple</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.11.10.2">Y</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.11.10.3">w/</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.11.10.4">0.24</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.11.10.5">0.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.11.10.6">-0.26</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.12.11">
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.11.1">Post-processing and multi-hop</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.11.2">Y</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.11.3">w/o</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.11.4">0.12</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.11.5">0.5</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.12.11.6">-0.38</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.13.12.1">Post-processing and multi-hop</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.13.12.2">Y</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.13.12.3">w/</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.13.12.4">0.24</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.13.12.5">0.59</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.13.12.6">-0.35</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We firstly tested the Llama3 8b pretrained model with or without RAG and prompt tuning in the domain of movie vs. finance and types of questions of simple vs. post-processing and multi-hop. The results of which are shown in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S4.T1" title="Table 1 â€£ 4. Results â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_tag">1</span></a>. Each test case ran 100 samples out of the 2.7k samples in CRAG. Surprisingly, we observed that adding more detailed instructions in the prompt actually dropped the overall performance significantly on the Llama3 8b pretrained model. One hypothesis is that the modelâ€™s performance is highly sensitive to the format of prompting and needs to be properly configured and fine-tuned.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">Given that the retrieval might not be correct, we also tried Gemini 1.5 pro with a 1 million token context window as a long context window models to see if feeding all the retrieved information to the LLMs would perform better than any RAG approaches. The results show no improvement, and we didnâ€™t further investigate on this.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">Table Â <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S4.T2" title="Table 2 â€£ 4. Results â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_tag">2</span></a> shows the overall results from our fine-tuned model, which achieved 0.096 with 323 samples from the online judging system. With this model, we achieved the highest score in Task 2 for the false premise problems (Fig. Â <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S4.F5" title="Figure 5 â€£ 4. Results â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_tag">5</span></a>). Finally, our hybrid approach results with cosine similarity threshold of 0.75 show that the score improved by 0.013 from 0.073 to 0.086 (Table Â <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S4.T3" title="Table 3 â€£ 4. Results â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_tag">3</span></a>) with fine-tuning and RAG combined. The accuracy increased by 0.026 and there was a slight increase in hallucination by 0.013.</p>
</div>
<div class="ltx_para" id="S4.p4">
<p class="ltx_p" id="S4.p4.1">Furthermore, Â <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S4.T4" title="Table 4 â€£ 4. Results â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_tag">4</span></a> shows some key examples of differences in predictions comparing the fine-tuned only model and the hybrid approach. For the first false premise question â€when did hamburg become the biggest city of germanyâ€, both models provide the answer â€invalid questionâ€, since the largest city in Germany is Berlin. For the second question which is a comparison question, both models also provide the same correct answer. For the third question related to movie domain, the fine-tuned model responds with â€i donâ€™t knowâ€, while the hybrid approach provides â€inceptionâ€ which is a correct answer. For the forth question related to movie domain, the fine-tuned model provides â€i donâ€™t know againâ€, while the hybrid approach provide a relevant answer including who are professionals relevant to the movie (e.g. the director Steve Carr), but not the complete list of the producers.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Fine-tuned Only Model with 323 Samples from Online Judging System (Task 3)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.2">Exact Accuracy</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.3">Accuracy</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.4">Hallucination</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.5">Missing</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.6">Total Score</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T2.1.2.2.1">Fine-tuned Only</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.2.2.2">0.111</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.2.2.3">0.152</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.2.2.4">0.056</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.2.2.5">0.793</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.1.2.2.6">0.096</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="349" id="S4.F5.g1" src="extracted/5849219/winner_score.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Team Future Got 64.6% Score in false_premise Question Type</figcaption>
</figure>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Score Comparison of Fine-tuned Only Model vs. Hybrid Approach with 300 Offline Samples (Task 1)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<td class="ltx_td ltx_border_tt" id="S4.T3.1.1.1.1"></td>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.2">Exact Accuracy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.3">Accuracy</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.4">Hallucination</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.5">Missing</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.6">Total Score</th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S4.T3.1.2.2.1">Fine-tuned Only</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.2.2">0.107</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.2.3">0.11</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.2.4">0.037</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.2.5">0.853</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.2.2.6">0.073</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.3.3">
<td class="ltx_td ltx_align_left" id="S4.T3.1.3.3.1">Hybrid Approach (cosine similarity threshold = 0.8)</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.3.2">0.117</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.3.3">0.123</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.3.4">0.043</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.3.5">0.833</td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.3.3.6">0.080</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S4.T3.1.4.4.1">Hybrid Approach (cosine similarity threshold = 0.75)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.4.4.2">0.113</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.4.4.3">0.136</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.4.4.4">0.05</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.4.4.5">0.813</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.4.4.6">0.086</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Answer Comparison of Fine-tuned Only Model vs. Hybrid Approach with 300 Offline Samples (Task 1)</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.1.1.1.1">Question</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T4.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.1.1.1.2.1">
<span class="ltx_p" id="S4.T4.1.1.1.2.1.1" style="width:85.4pt;">Ground Truth</span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.3">Fine-tuned Only Model</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T4.1.1.1.4">Hybrid Approach</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.2.1.1">when did hamburg become the biggest city of germany?</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.1.2.1.2.1">
<span class="ltx_p" id="S4.T4.1.2.1.2.1.1" style="width:85.4pt;">invalid question</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.3">invalid question</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.2.1.4">invalid question</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.3.2.1">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1.3.2.1.1">
<tr class="ltx_tr" id="S4.T4.1.3.2.1.1.1">
<td class="ltx_td ltx_align_left" id="S4.T4.1.3.2.1.1.1.1">which wta player had a higher singles ranking to end last year,</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.2.1.1.2">
<td class="ltx_td ltx_align_left" id="S4.T4.1.3.2.1.1.2.1">madison keys or daria kasatkina?</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.1.3.2.2.1">
<span class="ltx_p" id="S4.T4.1.3.2.2.1.1" style="width:85.4pt;">madison keys</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.2.3">madison keys</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.3.2.4">madison keys</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.4.3.1">what 2010 film was directed by christopher nolan?</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t" id="S4.T4.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.1.4.3.2.1">
<span class="ltx_p" id="S4.T4.1.4.3.2.1.1" style="width:85.4pt;">inception</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.4.3.3">i donâ€™t know</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.4.3.4">inception</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T4.1.5.4.1">who were the producers of the movie paul blart: mall cop?</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T4.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S4.T4.1.5.4.2.1">
<span class="ltx_p" id="S4.T4.1.5.4.2.1.1" style="width:85.4pt;">
<span class="ltx_tabular ltx_align_middle" id="S4.T4.1.5.4.2.1.1.1">
<span class="ltx_tr" id="S4.T4.1.5.4.2.1.1.1.1">
<span class="ltx_td ltx_align_left" id="S4.T4.1.5.4.2.1.1.1.1.1">adam sandler,</span></span>
<span class="ltx_tr" id="S4.T4.1.5.4.2.1.1.1.2">
<span class="ltx_td ltx_align_left" id="S4.T4.1.5.4.2.1.1.1.2.1">jack giarraputo,</span></span>
<span class="ltx_tr" id="S4.T4.1.5.4.2.1.1.1.3">
<span class="ltx_td ltx_align_left" id="S4.T4.1.5.4.2.1.1.1.3.1">kevin james,</span></span>
<span class="ltx_tr" id="S4.T4.1.5.4.2.1.1.1.4">
<span class="ltx_td ltx_align_left" id="S4.T4.1.5.4.2.1.1.1.4.1">todd garner,</span></span>
<span class="ltx_tr" id="S4.T4.1.5.4.2.1.1.1.5">
<span class="ltx_td ltx_align_left" id="S4.T4.1.5.4.2.1.1.1.5.1">barry bernardi</span></span>
</span></span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.1.5.4.3">i donâ€™t know</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T4.1.5.4.4">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1.5.4.4.1">
<tr class="ltx_tr" id="S4.T4.1.5.4.4.1.1">
<td class="ltx_td ltx_align_left" id="S4.T4.1.5.4.4.1.1.1">steve carr,</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.4.4.1.2">
<td class="ltx_td ltx_align_left" id="S4.T4.1.5.4.4.1.2.1">kevin james,</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.4.4.1.3">
<td class="ltx_td ltx_align_left" id="S4.T4.1.5.4.4.1.3.1">and</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.4.4.1.4">
<td class="ltx_td ltx_align_left" id="S4.T4.1.5.4.4.1.4.1">nick bakay</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We tried various approaches to alleviate the hallucinations of LLMs, including Retrieval-Augmented Generations (RAGs) and fine-tuning with domain-specific knowledge, while participating in the 2024 Meta KDD Cup with Comprehensive RAG Benchmark (CRAG) data. It turns out that RAG alone is not enough to perform well in the benchmark, and fine-tuning is needed to achieve a higher score. Our results show that the hybrid approach using both RAG and fine-tuning performs best in CRAG.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">For the basic RAG approach, we are uncertain whether the modelâ€™s accurate answers stem from the pretrained modelâ€™s prior knowledge or the retrieved data from the reference. At first glance, it appears that basic RAG does not significantly improve the final score. One possible reason is that the retrieved content, while relevant (cosine similarity score Â¿ 0.7), might not be useful because it is too basic, general, or vague. Additionally, improper prompting significantly reduces accuracy compared to using no prompts, likely because the pretrained model is misled by the instruction and loses its own prior knowledge to answer the question. More advanced RAG and prompt design should be tested to draw definitive conclusions.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">With further improvement in the hybrid RAG approach, focusing on the movie domain, it is clear that better quality of retrieved content helps improve the RAG results. In the Table Â <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S5.T5" title="Table 5 â€£ 5. Discussion â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_tag">5</span></a>, the second question shows that if the birth date is correct in the retrieved content, the prediction of Hybrid RAG Approach is correct. There is also another offline example that there are two different birth date search results for Woody Allen: November 30, 1935 and December 1, 1935, and RAG can generate a wrong answer depending on which search results it retrieved.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">The CRAG question dataset also contains many questions like â€In year <code class="ltx_verbatim ltx_font_typewriter" id="S5.p4.1.1">x</code>, which type <code class="ltx_verbatim ltx_font_typewriter" id="S5.p4.1.2">y</code> movie was recognized with the best type <code class="ltx_verbatim ltx_font_typewriter" id="S5.p4.1.3">y</code> movie in Oscarâ€. a common hallucination type occurs in this way: it provides a movie which was released in year <code class="ltx_verbatim ltx_font_typewriter" id="S5.p4.1.4">x</code>, but it won Oscar in year <code class="ltx_verbatim ltx_font_typewriter" id="S5.p4.1.5">x+1</code>. In the table Â <a class="ltx_ref" href="https://arxiv.org/html/2410.09699v1#S5.T5" title="Table 5 â€£ 5. Discussion â€£ Honest AI: Fine-Tuning â€Smallâ€ Language Models to Say â€I Donâ€™t Knowâ€, and Reducing Hallucination in RAG"><span class="ltx_text ltx_ref_tag">5</span></a>, it shows an example that â€the incrediblesâ€ was provided by RAG as the best animated feature film in Oscar 2004. However, the ground truth is â€finding nemoâ€, since â€Finding Nemoâ€ won Oscar in 2004, and â€The Incrediblesâ€ which was released in 2004 won Oscar in 2005.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5. </span>RAG Results are Sensitive to the Quality of Retrieved Content</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T5.1.1.1.1">Question</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T5.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.1.1.1.2.1">
<span class="ltx_p" id="S5.T5.1.1.1.2.1.1" style="width:56.9pt;">Ground Truth</span>
</span>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.3">
<table class="ltx_tabular ltx_align_middle" id="S5.T5.1.1.1.3.1">
<tr class="ltx_tr" id="S5.T5.1.1.1.3.1.1">
<td class="ltx_td ltx_align_left" id="S5.T5.1.1.1.3.1.1.1">Fine-tuned</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.1.1.3.1.2">
<td class="ltx_td ltx_align_left" id="S5.T5.1.1.1.3.1.2.1">Only Model</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.4">Hybrid RAG Approach</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.1.1.1.5">Relevant Retrieved Content</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.2.1.1">
<table class="ltx_tabular ltx_align_middle" id="S5.T5.1.2.1.1.1">
<tr class="ltx_tr" id="S5.T5.1.2.1.1.1.1">
<td class="ltx_td ltx_align_left" id="S5.T5.1.2.1.1.1.1.1">in 2004, which animated film</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.2.1.1.1.2">
<td class="ltx_td ltx_align_left" id="S5.T5.1.2.1.1.1.2.1">was recognized with the best</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.2.1.1.1.3">
<td class="ltx_td ltx_align_left" id="S5.T5.1.2.1.1.1.3.1">animated feature film oscar?</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_t" id="S5.T5.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.1.2.1.2.1">
<span class="ltx_p" id="S5.T5.1.2.1.2.1.1" style="width:56.9pt;">finding nemo</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.1.3">i donâ€™t know</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.1.4">the incredibles</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.1.2.1.5">
<table class="ltx_tabular ltx_align_middle" id="S5.T5.1.2.1.5.1">
<tr class="ltx_tr" id="S5.T5.1.2.1.5.1.1">
<td class="ltx_td ltx_align_left" id="S5.T5.1.2.1.5.1.1.1">Â¡DOCÂ¿ Superhero Best Animated</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.2.1.5.1.2">
<td class="ltx_td ltx_align_left" id="S5.T5.1.2.1.5.1.2.1">Movie Oscar The Incredibles (2004)</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.2.1.5.1.3">
<td class="ltx_td ltx_align_left" id="S5.T5.1.2.1.5.1.3.1">â€¦</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T5.1.3.2.1">
<table class="ltx_tabular ltx_align_middle" id="S5.T5.1.3.2.1.1">
<tr class="ltx_tr" id="S5.T5.1.3.2.1.1.1">
<td class="ltx_td ltx_align_left" id="S5.T5.1.3.2.1.1.1.1">when was the birth</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.2.1.1.2">
<td class="ltx_td ltx_align_left" id="S5.T5.1.3.2.1.1.2.1">of michael bay?</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S5.T5.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S5.T5.1.3.2.2.1">
<span class="ltx_p" id="S5.T5.1.3.2.2.1.1" style="width:56.9pt;">1965-02-17</span>
</span>
</th>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.1.3.2.3">i donâ€™t know</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.1.3.2.4">
<table class="ltx_tabular ltx_align_middle" id="S5.T5.1.3.2.4.1">
<tr class="ltx_tr" id="S5.T5.1.3.2.4.1.1">
<td class="ltx_td ltx_align_left" id="S5.T5.1.3.2.4.1.1.1">michael bay was born</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.2.4.1.2">
<td class="ltx_td ltx_align_left" id="S5.T5.1.3.2.4.1.2.1">on february 17, 1965.</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S5.T5.1.3.2.5">
<table class="ltx_tabular ltx_align_middle" id="S5.T5.1.3.2.5.1">
<tr class="ltx_tr" id="S5.T5.1.3.2.5.1.1">
<td class="ltx_td ltx_align_left" id="S5.T5.1.3.2.5.1.1.1">Â¡DOCÂ¿ What is the Michael Bay</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.2.5.1.2">
<td class="ltx_td ltx_align_left" id="S5.T5.1.3.2.5.1.2.1">date of birth?. . . . . .</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.2.5.1.3">
<td class="ltx_td ltx_align_left" id="S5.T5.1.3.2.5.1.3.1">The DOB for Michael Bay</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.2.5.1.4">
<td class="ltx_td ltx_align_left" id="S5.T5.1.3.2.5.1.4.1">was 17 Feb 1965</td>
</tr>
<tr class="ltx_tr" id="S5.T5.1.3.2.5.1.5">
<td class="ltx_td ltx_align_left" id="S5.T5.1.3.2.5.1.5.1">â€¦</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">With hallucination being one of the biggest challenges in applying LLMs to real-world applications, addressing different types of hallucination continues to require deeper and more extensive research and efforts. Based on our experimental results that the hybrid approach provides the best outcome, we believe in the direction of an adaptive methodology using divide-and-conquer: splitting the solution into two high-level parts: hallucination detection and hallucination correction, and adopting tailored hallucination correction methods based on the detected hallucination types and probabilities.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">In terms of detection, questions with low probabilities of hallucination can be questions in common domains, requiring simple logic, and/or about a past fact that doesnâ€™t change; questions with high probabilities of hallucination can often be in specialized domains, requiring complex reasoning and multi-hop steps, and/or fast-changing facts. Notice that the hallucination types and probabilities are a full spectrum, and they donâ€™t simply depend on the question itself, but also on the external context. For example, the question â€which country wins the most gold medals in Olympics Games Parisâ€ is fast-changing while the game is ongoing and will become a past fact after the game is over.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">In terms of correction, different strategies of answer correction can be adopted based on the detection results. For example, a real-time question will require real-time querying of the world knowledge (e.g. search engines that index the web in real-time); a specialized domain question could leverage external knowledge.</p>
</div>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">While both detection and correction remain extremely challenging, the proposed adaptive approaches using divide-and-conquer remain a promising direction for solving the hallucination problems in LLMs. In addition, due to limited time during the competition, the movie was primarily explored in the RAG hybrid approach, and other domains could also benefit from this approach.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Acknowledgments</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Great thanks to Ermo Wei, who shared invaluable LLM fine-tuning tips that significantly aided our fine-tuning efforts.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, etÂ al<span class="ltx_text" id="bib.bib2.3.1">.</span> 2020b.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.4.1">Advances in neural information processing systems</em> 33 (2020), 1877â€“1901.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and
Dario Amodei. 2020a.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Advances in Neural Information Processing Systems</em>, Vol.Â 33. Curran Associates, Inc., 1877â€“1901.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2024.

</span>
<span class="ltx_bibblock">Benchmarking Large Language Models in Retrieval-Augmented Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em> 38, 16 (Mar. 2024), 17754â€“17762.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen etÂ al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William Cohen. 2022.

</span>
<span class="ltx_bibblock">MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</em>. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 5558â€“5570.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Clark etÂ al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018.

</span>
<span class="ltx_bibblock">Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/1803.05457" title="">https://arxiv.org/abs/1803.05457</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dasigi etÂ al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, NoahÂ A. Smith, and Matt Gardner. 2021.

</span>
<span class="ltx_bibblock">A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dettmers etÂ al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.

</span>
<span class="ltx_bibblock">QLoRA: Efficient Finetuning of Quantized LLMs. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Advances in Neural Information Processing Systems</em>, Vol.Â 36. Curran Associates, Inc., 10088â€“10115.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao etÂ al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2024.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Large Language Models: A Survey.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2312.10997" title="">https://arxiv.org/abs/2312.10997</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ho etÂ al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xanh Ho, Anh-Khoa DuongÂ Nguyen, Saku Sugawara, and Akiko Aizawa. 2020.

</span>
<span class="ltx_bibblock">Constructing A Multi-hop QA Dataset for Comprehensive Evaluation of Reasoning Steps. In <em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Proceedings of the 28th International Conference on Computational Linguistics</em>. International Committee on Computational Linguistics, Barcelona, Spain (Online).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Joshi etÂ al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017.

</span>
<span class="ltx_bibblock">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. Association for Computational Linguistics, Vancouver, Canada, 1601â€“1611.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski etÂ al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, AndrewÂ M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.

</span>
<span class="ltx_bibblock">Natural Questions: A Benchmark for Question Answering Research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Transactions of the Association for Computational Linguistics</em> 7 (08 2019), 453â€“466.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis etÂ al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim RocktÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">Advances in Neural Information Processing Systems</em>, Vol.Â 33. Curran Associates, Inc., 9459â€“9474.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin etÂ al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zhen Lin, Shubhendu Trivedi, and Jimeng Sun. 2024.

</span>
<span class="ltx_bibblock">Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Transactions on Machine Learning Research</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma etÂ al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023.

</span>
<span class="ltx_bibblock">Query Rewriting in Retrieval-Augmented Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>. Association for Computational Linguistics, Singapore, 5303â€“5315.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">MÃ¶ller etÂ al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Timo MÃ¶ller, Anthony Reina, Raghavan Jayakumar, and Malte Pietsch. 2020.

</span>
<span class="ltx_bibblock">COVID-QA: A Question Answering Dataset for COVID-19. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020</em>. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nashid etÂ al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023.

</span>
<span class="ltx_bibblock">Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning. In <em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">2023 IEEE/ACM 45th International Conference on Software Engineering (ICSE)</em>. 2450â€“2462.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ouyang etÂ al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, etÂ al<span class="ltx_text" id="bib.bib18.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.4.1">Advances in neural information processing systems</em> 35 (2022), 27730â€“27744.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pang etÂ al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
RichardÂ Yuanzhe Pang, Alicia Parrish, Nitish Joshi, Nikita Nangia, Jason Phang, Angelica Chen, Vishakh Padmakumar, Johnny Ma, Jana Thompson, He He, and Samuel Bowman. 2022.

</span>
<span class="ltx_bibblock">QuALITY: Question Answering with Long Input Texts, Yes!. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>. Association for Computational Linguistics, Seattle, United States, 5336â€“5358.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rajpurkar etÂ al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.

</span>
<span class="ltx_bibblock">SQuAD: 100,000+ Questions for Machine Comprehension of Text. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</em>. Association for Computational Linguistics, Austin, Texas, 2383â€“2392.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid etÂ al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, etÂ al<span class="ltx_text" id="bib.bib21.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.05530" title="">https://arxiv.org/abs/2403.05530</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers and Gurevych (2019)</span>
<span class="ltx_bibblock">
Nils Reimers and Iryna Gurevych. 2019.

</span>
<span class="ltx_bibblock">Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>. Association for Computational Linguistics, Hong Kong, China, 3982â€“3992.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao etÂ al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhihong Shao, Yeyun Gong, Yelong Shen, Minlie Huang, Nan Duan, and Weizhu Chen. 2023.

</span>
<span class="ltx_bibblock">Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy. In <em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>. Association for Computational Linguistics, Singapore, 9248â€“9274.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talmor etÂ al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019.

</span>
<span class="ltx_bibblock">CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>. Association for Computational Linguistics, Minneapolis, Minnesota, 4149â€“4158.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi etÂ al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Harsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. 2022.

</span>
<span class="ltx_bibblock">â™« MuSiQue: Multihop Questions via Single-hop Question Composition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Transactions of the Association for Computational Linguistics</em> 10 (2022), 539â€“554.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xidong Wang, Guiming Chen, Song Dingjie, Zhang Zhiyi, Zhihong Chen, Qingying Xiao, Junying Chen, Feng Jiang, Jianquan Li, Xiang Wan, Benyou Wang, and Haizhou Li. 2024.

</span>
<span class="ltx_bibblock">CMB: A Comprehensive Medical Benchmark in Chinese. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>. Mexico City, Mexico, 6184â€“6205.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Antoine Yang, Arsha Nagrani, PaulÂ Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia Schmid. 2023.

</span>
<span class="ltx_bibblock">Vid2Seq: Large-Scale Pretraining of a Visual Language Model for Dense Video Captioning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> (2023), 10714â€“10726.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, RongzeÂ Daniel Gui, ZiranÂ Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, YifanÂ Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen tau Yih, and XinÂ Luna Dong. 2024.

</span>
<span class="ltx_bibblock">CRAG â€“ Comprehensive RAG Benchmark.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.04744" title="">https://arxiv.org/abs/2406.04744</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang etÂ al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and ChristopherÂ D. Manning. 2018.

</span>
<span class="ltx_bibblock">HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>. Association for Computational Linguistics, Brussels, Belgium.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu etÂ al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, and Meng Jiang. 2023.

</span>
<span class="ltx_bibblock">Generate rather than Retrieve: Large Language Models are Strong Context Generators. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">The Eleventh International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao etÂ al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jinming Zhao, Gholamreza Haffari, and Ehsan Shareghi. 2023.

</span>
<span class="ltx_bibblock">Generating Synthetic Speech from SpokenVocab for Speech Translation. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Findings of the Association for Computational Linguistics: EACL 2023</em>. Association for Computational Linguistics, Dubrovnik, Croatia, 1975â€“1981.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng etÂ al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
HuaixiuÂ Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, EdÂ H. Chi, QuocÂ V Le, and Denny Zhou. 2024.

</span>
<span class="ltx_bibblock">Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Research Methods</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1. </span>Prompt of Generating Answers with Domain in JSON format</h3>
<div class="ltx_para" id="A1.SS1.p1">
<span class="ltx_ERROR undefined" id="A1.SS1.p1.1">{mdframed}</span>
<p class="ltx_p" id="A1.SS1.p1.2">You are an agent that only outputs JSON. You are given a
Query and References. Do the following:</p>
</div>
<div class="ltx_para" id="A1.SS1.p2">
<p class="ltx_p" id="A1.SS1.p2.1">1. Determine the domain the query is about. The domain should be one of the following:
â€financeâ€, â€sportsâ€, â€musicâ€, â€movieâ€, â€encyclopediaâ€. If none of the domains apply, use â€otherâ€. Use â€domainâ€ as the key in the result json.</p>
</div>
<div class="ltx_para" id="A1.SS1.p3">
<p class="ltx_p" id="A1.SS1.p3.1">2. Answer the question in as few words as possible. Please follow these guidelines when formulating your answer.
If the question contains a false premise or assumption, answer â€invalid questionâ€.
If you are uncertain or donâ€™t know the answer, respond with â€I donâ€™t knowâ€. Use â€answerâ€ as the key in the result json.</p>
</div>
<div class="ltx_para" id="A1.SS1.p4">
<p class="ltx_p" id="A1.SS1.p4.1">### Question
{query}</p>
</div>
<div class="ltx_para" id="A1.SS1.p5">
<p class="ltx_p" id="A1.SS1.p5.1">### References
{references}</p>
</div>
<div class="ltx_para" id="A1.SS1.p6">
<p class="ltx_p" id="A1.SS1.p6.1">### Answer</p>
</div>
<div class="ltx_para" id="A1.SS1.p7">
<p class="ltx_p" id="A1.SS1.p7.1"><span class="ltx_text ltx_font_bold" id="A1.SS1.p7.1.1">Example Output</span>
<span class="ltx_ERROR undefined" id="A1.SS1.p7.1.2">{mdframed}</span>
{</p>
</div>
<div class="ltx_para" id="A1.SS1.p8">
<p class="ltx_p" id="A1.SS1.p8.1">â€domainâ€: â€movieâ€,
â€answerâ€: â€Amyâ€
}

<br class="ltx_break"/>â€â€â€

<br class="ltx_break"/># Step 1: Determine the domain

<br class="ltx_break"/>domain = â€movieâ€

<br class="ltx_break"/># Step 2: Answer the question

<br class="ltx_break"/>answer = â€Amyâ€

<br class="ltx_break"/># Create the result JSON

<br class="ltx_break"/>result = {â€domainâ€: domain, â€answerâ€: answer}

<br class="ltx_break"/>return result</p>
</div>
<div class="ltx_para" id="A1.SS1.p9">
<p class="ltx_p" id="A1.SS1.p9.1">Since sometimes the model generate additional text besides JSON output, additional code is written to extract the first JSON in the output to solve the problem.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2. </span>An example of hallucination</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">For example, a simple movie question â€can you tell me the date that Randall Wallace was born?â€(Ground truth: 1949-07-28), the model answers the same question repetitively:</p>
</div>
<div class="ltx_para" id="A1.SS2.p2">
<span class="ltx_ERROR undefined" id="A1.SS2.p2.1">{mdframed}</span>
<p class="ltx_p" id="A1.SS2.p2.2">Â¿Â¿According to the provided information, Randall Wallace was born on July 28, 1949;

<br class="ltx_break"/>Â¿Â¿According to my sources, Randall Wallace, the American screenwriter and director, was born on July 28, 1949;

<br class="ltx_break"/>Â¿Â¿I can confirm it now: Randall Wallace was born on November 7, 1948.

<br class="ltx_break"/>Â¿Â¿Randall Wallaceâ€™s birthday is September 1, 1948.

<br class="ltx_break"/>Â¿Â¿Iâ€™ve got it nailed down now! Randall Wallace was indeed born on November 7, 1949

<br class="ltx_break"/>Â¿Â¿I finally have it exact: Randall Wallace was born on July 1, 1948 (no, wait!), I meanâ€¦ September 1, 1949 (no), actually it is November 7, 1946. Yes, thatâ€™s correct!</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Oct 12 05:35:16 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
